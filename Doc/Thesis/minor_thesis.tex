% RMIT University School of CS&IT
% Minor thesis template
% S.M.M. (Saied) Tahaghoghi, 2004
\documentclass[11pt,twoside]{report}
\usepackage{a4wide,caption,epsfig,fancyheadings,natbib,url,multirow,makecell}

% Place the correct values here
%Set to the original submission date when submitted amended thesis
\newcommand{\SubmissionDate}{\today}
\newcommand{\student}{Bin Lu}
\newcommand{\supervisor}{Dr Jenny Zhang, Dr Amanda Kimpton, Dr Daryl D'Souza}
\newcommand{\topic}{Topic Modelling of Patient Opinion}
\newcommand{\school}{School of Computer Science and Information Technology}
\newcommand{\program}{Masters of Computer Science}
\newcommand{\institution}{Royal Melbourne Institute of Technology}

% Use the remark command to highlight text for discussion
\newcommand{\remark}[1]{{\bf \em [\marginpar{$\Leftarrow$}#1]}}

\renewcommand{\leftmark}{\student}
\renewcommand{\rightmark}{\topic}
%\setlength{\headrulewidth}{0pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.5ex plus 0.3ex}

% This is the line spacing - set to 2 for draft submission to
% supervisor, 1.3 for the final submission
\renewcommand{\baselinestretch}{2.00}

\renewcommand{\captionfont}{\it}
\raggedbottom

%For Natbib Author, year citation format
% - the opening bracket symbol, default = (
% - the closing bracket symbol, default = )
% - the punctuation between multiple citations, default = ;
% - the letter n for numerical style, or s for numerical superscript
%   style, any other letter for author year, default = author year;
% - the punctuation that comes between the author names and the year
% - the punctuation that comes between years or numbers when common author lists are suppressed (default = ,);
\bibpunct{[}{]}{;}{a}{,}{;}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{{\Large\bf \topic}}
\author{
A minor thesis submitted in partial fulfilment of the requirements for the degree of
\\\program\\*[10mm]
%\epsfig{figure=Figs/rmit-coa.epsf,width=5cm}
\\\student
\\\school
\\Science, Engineering, and Technology Portfolio,
\\\institution
\\Melbourne, Victoria, Australia
}
\maketitle
\thispagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Declaration}

This thesis contains work that has not been submitted previously, in
whole or in part, for any other academic award and is solely my
original research, except where acknowledged.

This work has been carried out since March 2014, under the
supervision of {\supervisor}.

\paragraph{}
\vspace{5cm}\noindent \\\student \\
\school\\
\institution\\
\SubmissionDate

\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Acknowledgements}

TODO:THANKS!

\tableofcontents
\listoffigures
\listoftables

\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
Topic models have been widely used to identify topics in text collections. Many studies tried to fit variety of data with existing model or extension of existing model. Since each type of the data has its own characteristic, it is not possible to find a solution to suite all. Current studies cover consumer reviews, blogs, or even twitter. It left few missing pieces, for example healthcare reviews. We study the data from a website called Patient Opinion, it's unique data structure gives us advantage to improve the topic modelling result without modifying baseline model. To achieve this objective, particular fields from user input from Patient Opinion are retrieved, this collection of terms is used to filter out noise from baseline model. Also each term in topic will get ranked by \textit{tf-idf} score in the scope of result topic. Our evaluation demonstrate the effectiveness of topic coherence improvement by reducing the noise. The effectiveness of ranking is overlooked due to the limitation of evaluation method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
Publicly available opinions and service feedback provide valuable information for decision making for both service providers and consumers. With the help of websites, blogs, forums and social networks, it has never been easier to express opinions and leave feedback. Analyzing the opinions becomes a challenge, not only because of the quantity of the data, most opinion from general users are free form text. The massive quantity of the data won’t be effectively used until there is a systematically approach of analyzing and summarizing, in this project we focus on topic modeling side, aiming to discover a set of terms that can form a topic, hence with the topics the collection of document can be easily categorized or summarized. Many techniques have been proposed to solve this problem. Most previous studies focus on analyzing product reviews. We are interested to discover a model that suite service reviews. More specifically, reviews relate to health system. Study shows the effective governance is increasingly recognized as pivotal to improvements in healthcare quality (\cite{ref6}), moreover current issue of effectiveness of the authority is affected by insufficient resource and inadequate information received (\cite{ref5}). 

The official authority called Australian Health Practitioner Regulation Agency (AHPRA) is responsible to monitor performance and conduct of health practitioners across 14 health professions. Since 2010, laws in all Australian states and territories require health practitioners to report all "notifiable conduct" that comes to their attention to the AHPRA. The report targets all registered health practitioners in Australia that includes doctors, nurses, dentists and practitioners from 11 allied health professions. And the obligation assigned to employers, education providers and health practitioners. The misconducting will be reported if there is a reasonable belief that instance is notifiable. The regime itself has sparked controversy and debate, supporters believes it encourages employers and practitioners to address poor performance, and improves surveillance of threats to patient safety. Concerns have also been raised about the subjectivity of reporting criteria. The level of "notifiable" could be influenced by varieties of factors and can be very subjective. 
Apart from the official report AHPRA is getting internally from health system, there are also publicly available stories, feedbacks and opinions from patient, which could address the same issues in another perspective. 

The object we are going to study is www.patientopinion.org.au, it is a publicly available healthcare forum. It allows user to post their own healthcare related story,  the story can be positive or negative or a bit from both side. Although the story body is free form text, user still has to follow a certain template while submit the story. There is a unique feature of the data from Patient Opinion, user could specify the key word while submitting the story, which we could treat as pre-defined terms for topics, and they will be used weight the terms that generate by the topic model algorithm. 

MDK-LDA model proposed by Chen (\cite{ref24}) , the method extends the Latent Dirichlet Allocation (\cite{ref25}), the later one becoming the standard method in topic modelling and been extended in variety ways. The basic idea of LDA is treat each document in a collection as a vector of word count, each document is represented as a probability distribution over a number of topics, while each topic is represented as a probability distribution over a number of words. MDK-LDA introduces a new latent variable s in LDA to model s-sets. Each document is an admixture of latent topics while each topic is a probability distribution over s-sets. Another approach is Aspect-based Summarization (\cite{ref11}), it is usually composed of three main tasks: aspect identification, sentiment classification, and aspect rating. Generally this model is used to analyzing product review, it is designed to effectively retrieve features and sentiment for products.

Due to the unique characteristic of the data from ‘Patient Opinion’, we could improve existing algorithm with the additional information from the data set. LDA has been approved a very effective  model, and been used as a based model in many topic modelling studies. We choose LDA as our base model, and incorporate unique feature in ‘Patient Opinion’, specifically the section of ‘What’s Good’ and ‘What could be improved’. These two sections are filled in by user while submitting the story, the template is provided by the website. Generally this will be the main topic or features user want to give feedback about in the story. And we assume user labelled story 100% accurate.
\section{Research Question}
\begin{itemize}
\item How to use user specified key words to improve the performance and accuracy in topic modelling.
\end{itemize}

\section{Research Contribution}
The project has made the following contribution to the field of topic modelling by using the LDA as base framework:
By introducing the user specified terms, the number of term that form each topic could be reduced significantly while retain the quality of the topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Works}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Topic Modelling}
Also known as Latent Dirichlet Allocation or discrete PCA is a Bayesian graphical model for text document collections represented by bags-of-words (\cite{ref26}, \cite{ref25}, \cite{ref30}, \cite{ref31}). The model allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. Generally, only a small number of words have high likelihood in each topic and each document only presents certain number of topics. Following is the equation of collapsed Gibbs sampling:
\begin{equation}
p(z_{id}=t\mid x_{id}=w,Z^{\neg id}) \alpha
\end{equation}
\begin{equation}
\frac{N_{wt}^{\neg id} + \beta}{\sum_{w}N_{wt}^{\neg id} + W\beta} \frac{N_{wt}^{\neg id} + \alpha}{\sum_{w}N_{wt}^{\neg id} + T\alpha}
\end{equation}

where $z_{id}=t$ assigns topic t with $i^{th}$ word in document d, and word w currently observed indicated by $x_{id}=w$.
$Z^{\neg id}$ is the vector of all topic assignments not including the current word. $N_{wt}$ represent integer count arrays, and $\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions, $\beta$ is the per-topic word distribution.

% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Multi-Domain Prior Knowledge in Topic Modelling}
As mentioned before, LDA is a powerful topic modelling framework, however recent studies found that these unsupervised models may not produce topics that conform to the user's existing knowledge(\cite{ref24}). Chen et al (\cite{ref24}) proposed a novel knowledge-based model, called MDK-LDA, which is capable of using prior knowledge from multiple domains to help topic modelling in the new domain. A new latent variable 's' is added to model the s-set, each document represent admixture of latent topics while each topic is a probability distribution over s-set. MDK-LDA uses s-set to distinguish topics in multiple senses. For example the word light can be represented by two s-set: S1 \{light, heavy, weight\} and S2 \{light, bright, luminance\}, if light co-occurs with bright or luminance it will be assigned to S2.
In conclusion, MDK-LDA outperforms base LDA model, in other word, the extra information improved the quality of result. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Improve topic coherence by using user input}

Although LDA provides a powerful framework for extracting latent topics in text document, but sometimes learned topics are lists of words that do not convey much useful information (\cite{ref26}). Some extrinsic evaluation has been used to demonstrate the effectiveness of the learned topic in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively (\cite{ref27}). To solve the problem, base LDA model had been extended either by incorporating human judgement in to the model-learning framework or creating a computational proxy that simulates human judgements (\cite{ref28}), for example the MDK-LDA model (\cite{ref24}) we introduced in section 2.On the other side, solutions like MDK-LDA require certain level of human interaction, which limit the size of the training data. And also part of the training constraints are defined by domain experts, which means the result could contain certain level of subjective factors.
Due to the unique characteristic of the data of Patient Opinion, we have the opportunity to minimize the input from third party during training phase by employing use user input to simulate human judgment, hence to produce the topic modelling result with less subjective factors.


% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Description of Data}
Data from Patient Opinion contains many information, however we only interested in few parts of them in our project Figure \ref{figure:Figure1} 1) The title of the story, it’s the summary of story by the user. 2) The author role and time of the post, the role could be the patient, patient’s relative, carer or doctor. 3) The more about section is from website moderator, it inserts relevant tags to the story. 4) \& 5) are the most important fields to our project, these field are inserted by the user, the fields indicate what user thinks the story is about, and we use these fields to simulate user judgement in topic modelling. 
\begin{figure}[h]
    \begin{center}
    \epsfig{figure=Figs/story_sample.jpg,width=16.7cm}
    \caption
    [Patient Opinion Story Sample]
    {
    Patient Opinion Story Sample
    \label{Figure1}
    }
    \end{center}
\end{figure}

\begin{figure}[h]
    \begin{center}
    \epsfig{figure=Figs/story_sample_source1,width=16.7cm}
    \caption
    [Patient Opinion Story Sample Source 1]
    {
    Patient Opinion Story Sample Source 1
    \label{Figure2}
    }
    \end{center}
\end{figure}

\begin{figure}[h]
    \begin{center}
    \epsfig{figure=Figs/story_sample_source2,width=10.7cm}
    \caption
    [Patient Opinion Story Sample Source 2]
    {
    Patient Opinion Story Sample Source 2
    \label{Figure3}
    }
    \end{center}
\end{figure}
Each story also includes some non-mandatory fields which not in the screenshot but provide very important information, for example the location information, some stories pinpoint to a particular hospital or clinic, most stories at least have state information. A quick overview of the data shows the potential of using the data from public forum for health system. Patient Opinion Australia established in 2012, so by the time we collect the data, it contains 624 stories in total. It's allied site Patient Opinion UK was founded in 2005, by today it has more than 80,000 stories, as the scope of this project focus on Australian health system, the data from UK will not be used, but the indication is clear that health system is attracting feedback from general public, the information collected from those sources could help to improve the system. Taking the example of Patient Opinion Australia, we collected 659 unique terms in user specified field out of 624 stories, the most frequent terms are: "care" appears 399 times in 278 stories, "service" appears 150 times in141 stories, staff appears 148 times in 116 stories and hospital appears 141 times in 118 stories. Break above number down to two groups "Good" and "Need to Improve", we have 412 user specified terms in "Good" out of 467 stories, while the "Need to Improve" owns 408 terms in 264 stories, one interesting observation is the general order of term frequency in "Good" group matches overall count, while "Need to Improve" group shows some disputes, instead of "service" and "staff", "hospital, doctor, communication" seats right after "care" in this group. It suggests stories relate to "service, staff" more likely get positive feedback compares to "hospital, doctor, communication". But both side shows the interests from general public, we do not want to overlook the topics from positive side. So when we feed the data to topic modelling algorithm, we do not distinguish the semantic meaning of the terms, we treat all user specified terms as a single group. 

\begin{table}[h]
\caption{Overall Term Count Over States}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c |c | c | c |}
\hline\hline
Term & TF & DF & NSW & VIC & ACT & TAS & QLD & SA & NT & WA \\
\hline
care & 399 & 278 & 61 & 28 & 4 & 3 & 114 & 21 & 2 & 5 \\
\hline
service & 150 & 141 & 51 & 7 & 1 & 0 & 51 & 5 & 1 & 5 \\
\hline
staff & 148 & 116 & 32 & 12 & 1 & 0 & 45 & 4 & 0 & 4 \\
\hline
hospital & 141 & 118 & 18 & 18 & 4 & 3 & 47 & 10 & 1 & 4 \\
\hline
doctor & 102 & 84 & 16 & 7 & 2 & 1 & 40 & 6 & 1 & 0 \\
\hline
\end{tabular}
\label{table:OverAllCount}
\end{table}

Further look into the numbers in state level, the overall count shows in Table \ref{table:OverAllCount}. NSW and QLD are the most active states. Table \ref{table:GoodCount} shows the "Good" group. 

\begin{table}[h]
\caption{Group "Good" Term Count Over States}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c |c | c | c |}
\hline\hline
Term & TF & DF & NSW & VIC & ACT & TAS & QLD & SA & NT & WA \\
\hline
care & 250 & 178 & 48 & 13 & 2 & 2 & 73 & 11 & 2 & 6 \\
\hline
service & 125 & 119 & 47 & 5 & 1 & 0 & 42 & 4 & 1 & 5 \\
\hline
staff & 124 & 98 & 30 & 1 & 1 & 0 & 35 & 3 & 0 & 3 \\
\hline
hospital & 80 & 68 & 8 & 9 & 1 & 2 & 31 & 5 & 1 & 2 \\
\hline
information & 54 & 49 & 42 & 0 & 0 & 0 & 4 & 1 & 0 & 1 \\
\hline
doctor & 46 & 40 & 11 & 2 & 1 & 0 & 17 & 4 & 1 & 0 \\
\hline
\end{tabular}
\label{table:GoodCount}
\end{table}

The term "information" has been mentioned 54 times out of 49 stories, which 42 stories are from NSW, this could suggest patients in NSW are more satisfied in "information" related topics than the rest of country. Table \ref{table:ImproveCount} shows the result for "Need to Improve" group, the term frequency of  "care, doctor, communication, staff" relate to QLD are significantly higher than the rest of the country. 

\begin{table}[h]
\caption{Group "Need to Improve" Term Count Over States}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c |c | c | c |}
\hline\hline
Term & TF & DF & NSW & VIC & ACT & TAS & QLD & SA & NT & WA \\
\hline
care & 149 & 115 & 16 & 15 & 2 & 1 & 47 & 12 & 1 & 3 \\
\hline
hospital & 61 & 54 & 11 & 8 & 3 & 0 & 15 & 4 & 1 & 1 \\
\hline
doctor & 56 & 44 & 5 & 5 & 1 & 1 & 23 & 2 & 0 & 0 \\
\hline
communication & 26 & 26 & 2 & 1 & 1 & 0 & 15 & 5 & 0 & 0 \\
\hline
service & 25 & 23 & 4 & 2 & 0 & 0 & 9 & 1 & 0 & 0 \\
\hline
staff & 24 & 22 & 2 & 1 & 0 & 0 & 12 & 1 & 0 & 1 \\
\hline
\end{tabular}
\label{table:ImproveCount}
\end{table}
All the number we had showed are individual cases, in reality each incident could occupy few terms, and different combination of terms could form a meaningful topic. Our project intend to employ traditional topic modelling technique to discover the latent topics among documents, and then increase the accuracy of the result with the help of user specified keywords.



% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Preprocessing of Data}

To be able to feed the data to Mallet, the original data need to be normalized. The story body and story ID are extracted from website. All stories are condensed into one single file, which each story are formatted to a single line start with story ID, the leading story ID is used by Mallet for labelling the composition result. Also some internal data structure also defined for post processing, everything been converted to lowercase, collect all unique user specified key words. This collection is used to filter out the words in each topic that generated by LDA at a later stage. A list of related document ID to each word also collected, (see Figure \ref{figure:Figure4}) and indexed for fast look up for document frequency.

\begin{figure}[tp]
    \begin{center}
    \epsfig{figure=Figs/data_sample1.jpg,width=8cm}
    \caption
    [Patient Opinion Story Sample]
    {
    Patient Opinion Story Sample
    \label{Figure4}
    }
    \end{center}
\end{figure}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Using user input to improve topic modelling result}

Topics learned from LDA sometimes don’t convey much useful information, sometime it is caused by overfeeding the result set, for example it will include top 20 words for each topic (based on the settings, the total number in each topic can be configured), some words may not make any sense in current topic but statistically significant to the topic. Our goal  is try to use user input to reduce the noise while retaining as much information as possible to describe or label the topic. The generative process is given as follows: 
\begin{enumerate}
\item Collect unique words from user specified field as \textit{S} set.
\item Generate a set of topics \textit{T} with LDA model.
\item Calculate result set \textit{R} as: for each topic $t\in\{1,...,T\}, r_n = t_n \cap S$
\item Treat the result T as collection of document, calculate idf for each term with $idf(t, D) = log\frac{N}{\mid \{d\in D : t\in D\}\mid}$, where N = 100 represent the 100 topics generated with LDA. The original tf-idf method is not used, for each term the tf for current document always equals to 1. 
\item Re-arrange terms in set R with idf score, so the more significant word appear in front of each topic.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments and Result}

We collected all 624 stories from Patient Opinion by August 2014. The count of unique user specified term is 659. 100 topics are generated using Mallet \footnote{http://mallet.cs.umass.edu/} with setting of optimize interval equals to 20.  The number of unique terms in the result set R is 527, compare to 1440 in the original T set. 
Table \ref{table:Composition} Shows the topic composition, which is the per topic probability distribution over documents. Rows represent documents with document ID in first column and the remaining columns represent topic probability distributions over current document. 
\begin{table}[h]
\tiny
\caption{Example of Composition}
\centering
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c |}
\hline\hline
Doc ID & Topic Index & Composition & Topic Index & Composition & ... & Topic Index & Composition & Topic Index & Composition\\
\hline
58954 & 61 & 0.1171875 & 91 & 0.0390625 &...& 78 & 0.0234375 & 72 & 0.0234375\\
\hline
58832 & 83 & 0.076388889 & 78 & 0.048611111 &...& 10 & 0.048611111 & 71 & 0.034722222\\
\hline
58953 & 83 & 0.077380952 & 65 & 0.053571429 &...& 29 & 0.041666667 & 60 & 0.029761905\\
\hline
58956 & 78 & 0.025 & 76 & 0.025 &...& 65 & 0.025 & 62 & 0.025\\
\hline
58834 & 42 & 0.065517241 & 11 & 0.065517241 &...& 58 & 0.037931034 & 44 & 0.037931034\\
\hline
58710 & 12 & 0.108208955 & 18 & 0.063432836 &...& 90 & 0.041044776 & 71 & 0.041044776\\
\hline
58952 & 91 & 0.044642857 & 73 & 0.026785714 &...& 59 & 0.026785714 & 36 & 0.026785714\\
\hline
58830 & 94 & 0.108490566 & 80 & 0.051886792 &...& 99 & 0.04245283 & 71 & 0.04245283\\
\hline
58951 & 93 & 0.0625 & 81 & 0.052884615 &...& 0 & 0.052884615 & 79 & 0.043269231\\
\hline
58719 & 51 & 0.27238806 & 27 & 0.063432836 &...& 42 & 0.026119403 & 22 & 0.026119403\\
\hline
58716 & 77 & 0.041666667 & 90 & 0.025 &...& 73 & 0.025 & 62 & 0.025\\
\hline
58718 & 83 & 0.242307692 & 47 & 0.05 &...& 71 & 0.042307692 & 11 & 0.034615385\\
\hline
58839 & 25 & 0.070512821 & 63 & 0.032051282 &...& 59 & 0.032051282 & 58 & 0.032051282\\
\hline
58717 & 43 & 0.050660793 & 57 & 0.046255507 &...& 92 & 0.04185022 & 72 & 0.04185022\\
\hline
58723 & 91 & 0.028301887 & 68 & 0.028301887 &...& 35 & 0.028301887 & 99 & 0.009433962\\
\hline
58965 & 83 & 0.097014925 & 1 & 0.037313433 &...& 77 & 0.02238806 & 72 & 0.02238806\\
\hline
\end{tabular}
\label{table:Composition}
\end{table}


The total composition score for each topic is calculate with the help from term to document ID index we build before. See Table \ref{table:SumComposition}

\begin{table}[h]
\caption{Sum of Composition}
\centering
\begin{tabular}{c c c}
\hline\hline
Topic Index & Original Composition & Filtered Composition\\
\hline
1 & 3.920362 & 3.382284\\
2 & 3.742091 & 3.275150\\
3 & 4.396231 & 3.930039\\
4 & 4.275353 & 3.603821\\
5 & 4.569670 & 4.444751\\
6 & 3.153133 & 2.775077\\
7 & 3.368214 & 2.622281\\
8 & 4.541646 & 4.018145\\
9 & 5.399709 & 5.194911\\
10 & 4.498450 & 3.711763\\
11 & 4.255079 & 4.101976\\
12 & 6.755018 & 6.489172\\
\hline
\end{tabular}
\label{table:SumComposition}
\end{table}

Clearly, the total composition of T is expected to greater than it's subset R. The average difference of composition between T and R over 100 topics is 1.0829, there are 58 topics has the difference below the average, we consider the probability of distribution of the topic over the documents still significant in that 58 topics. Samples are selected from remaining topics, see Table \ref{table:SampleTopics}. 
1116 out of original 1440 terms appears once in T set. So more than half of the term has the highest idf value 2 in the data set. 236 terms appear twice with idf=1.7 and 55 terms appear 3 times with idf = 1.5. There are significant variance in the result topics. Some topics may remain the existing order or only have one word shifted, some may look very differently.  For example, \{program, kate, lifestle, meet, included, learnt, learning, relationship\}, only $idf_{kate} = 1.7$, the idf for the rest equals to 2. So the new topic becomes \{program, kate, meet, included, learnt, learning, relationship\}. Another example \{bed, family, care, staff, time, attending, unit, palliative\}, $idf_{care}=1.3, idf_{staff}=1.2 and idf_{palliative}=2$, the rest has idf equals to 1.7, then the new topic should look like \{palliative, bed, family, time, attending, unit, care, staff\}. 

\begin{table}[h]
\tiny
\caption{Sample topics}
\centering
\begin{tabular}{|c|c|}
\hline\hline
Original & Filtered\\
\hline
\makecell{program healthy kate lifestyle sessions eat meet programme\\
	 included organised held healthier encouraging learnt foods\\
	 beneficial learning relationship handle} & {program kate lifestyle meet included learnt learning relationship} \\
\hline

\makecell{gp local recently records government copy prescription\\
	 paper multiple tasmania gps referring avail beginning calls\\
	  surprised cairns super shared } & {gp local records government prescription paper gps cairns}\\
\hline

\makecell{physio gp mri injury follow shoulder xray week asked\\
	 hospital discussed full physiotherapist neck stand \\
	 complaining neurologist princess forte} & {physio gp mri xray hospital full physiotherapist neck}\\
\hline

\makecell{call waiting phone told back called list unit\\
	 rang ring explain apparently assumed clerk calling\\
	 noticed mcewin lyell requested} & {call waiting phone back list unit clerk calling}\\
\hline
  
\makecell{father bed family care appears staff time\\
	 attending difficult dad speak comfort unit incident\\
	 law visitor awake gosford palliative} & {bed family care staff time attending unit palliative}\\
\hline
	
\makecell{time advised team contact causing consultant tumour\\
	 professional manner independent safe stressful arrival\\
	 note closed usual considerate empathetic seizures} & \makecell{time team contact consultant professional manner\\ independent empathetic}\\
\hline

\makecell{looked experience er bad partner full approach worry\\
	 give free skills male chronic terrible running provider\\
	  building drive welcomed} & {looked experience er partner full approach skills male building}\\
\hline

\makecell{night stay thing major hospital admission support\\
	 suggested accommodation fully sydney unable relatives\\
	 sleep developed tuesday staff added environment} & \makecell{night stay hospital admission accommodation relatives\\ sleep staff environment}\\
\hline

\makecell{waiting wait hours room hour area waited reception\\
	 number temperature hurt remember panadol minutes geelong\\
	 sunday impressed time er} & {waiting wait hours room area reception number time er}\\
\hline

\makecell{child issues jean aboriginal helped understanding\\ 
	knowledge school minds behaviour hay mighty woods anger\\ 
	louise clinician strategies interaction love} & \makecell{child jean aboriginal understanding knowledge\\ 
	school minds behaviour woods strategies}\\
\hline

\makecell{public brisbane system live mater hospital pa\\ 
	run booking advise toowoomba meds qld health expect\\ 
	west lift weekend weak} & \makecell{public brisbane system hospital run booking meds\\ health west lift}\\
\hline

\makecell{health community sarah local support primary\\ 
	medicare rural group libby people murrumbidgee provide\\ 
	part clients topics art groups guest} & \makecell{health community sarah local primary medicare rural\\ group people murrumbidgee clients guest}\\
\hline
\end{tabular}
\label{table:SampleTopics}
\end{table}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Topic Coherence Evaluation}
Apart from quantitative and qualitative evaluation as above, evaluating topic coherence is a component of the larger question of what are good topics, what characteristics of a document collection make it more amenable to topic modelling, and how can the potential of topic modelling be harnessed for human consumption (\cite{ref27}). The topic coherence is measured with Pointwise Mutual Information (\cite{ref32}) (PMI) score:
\begin{equation}
PMI-Score(w) = (\frac{N^{2}-N}{2})^{-1}\sum PMI(w_{i}, w_{j}), ij\in \{1...N\}
\end{equation}
\begin{equation}
where PMI(w_{i}, w_{j}) = log \frac{P(w_{i}, w{j})}{P(w_{i})P(w_{j})},
\end{equation}
Since the number of term that form each topic isn't normalized, we calculate the average of the topic, where N is the number of terms in that topic. $(\frac{N^{2}-N}{2})^{-1}$ gives the number of distinct pairs in N.
The measure is symmetric $P(w_{i}, w{j}) = P(w_{j}, w{i})$, which means we only measure the difference of topic coherence between original topic and filtered topic.

Table \ref{table:PMIScores} Shows some samples of the PMI scores, plus the difference between two PMI in last column. As we can see from the result, the majority of the topics result in improvement of coherence, and the overall average is 0.297. A paired-t test shows the $P=3.76e^{-22}$, which means the filtered topic is significant from the original one.

\begin{table}[h]
\caption{PMI Scores}
\centering
\begin{tabular}{c c c c}
\hline\hline
Topic Index & Original Topic PMI Scores & Filtered Topic PMI Scores & Difference\\
\hline
1 & 1.864771 & 2.040466 & 0.175695\\
2 & 1.844573 & 2.305665 & 0.461092\\
3 & 1.543335 & 1.928804 & 0.385469\\
4 & 2.145729 & 2.333656 & 0.187927\\
5 & 1.912059 & 2.195632 & 0.283573\\
6 & 1.854318 & 2.341428 & 0.48711\\
7 & 1.748795 & 2.33346 & 0.584665\\
8 & 1.899982 & 2.624272 & 0.72429\\
9 & 1.752184 & 1.80262 & 0.050436\\
10 & 2.152952 & 2.371954 & 0.219002\\
11 & 1.986177 & 2.120463 & 0.134286\\
12 & 1.776924 & 2.030256 & 0.253332\\
\hline
\end{tabular}
\label{table:PMIScores}
\end{table}

\begin{figure}[tp]
    \begin{center}
    \epsfig{figure=Figs/pmi.jpg,width=8cm}
    \caption
    [PMI-Score]
    {
    PMI-Score
    \label{Figure5}
    }
    \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion and Future Work}
This study shows the possibility of using user input to improve topic coherence in topic modelling of healthcare related blogs. To the best of our knowledge, this has not been done before. We proposed a method that use user input words as a filter for the result from baseline LDA model. We successfully reduced the number terms in each topic while still keep the topic meaningful, the terms that been omitted can be considered as noise, which means the documents they associate to do not significantly contribute to the possibility distribution over the topic, this is evaluated by the total composition score. Hence the overall topic coherence is improved as less noise in the topic. Furthermore, we experiment using tf-idf to re-rank the terms in each topic. Unfortunately, PMI-score evaluation is symmetric, which means the order of each term in topic isn't taken in count. We couldn't find an existing statistical model to fit in the evaluation. 
Our method of ranking the terms by idf isn't ideal, since the term frequency for each term for in a topic always equals to 1, this approach is reasonable for step 1. With more resources in the future work, this approach could be expended to count term frequency in the scope of whole collection of documents, and idf still from the topic list. If the scope of the factor expended to the whole collection, it is also reasonable to rank the topics not only the terms in each topic, hence the model could make suggestions of which topic is more likely an important one.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{abbrvnat}
\bibliography{Bib/strings,Bib/main}
\end{document}
\end{document}
